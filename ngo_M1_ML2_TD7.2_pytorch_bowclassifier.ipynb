{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20f797f2110>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modified version of notebook by Robert Guthrie\n",
    "# https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#sphx-glr-beginner-nlp-deep-learning-tutorial-py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from random import shuffle\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic way of creating a neural network in Pytorch : inheriting from the nn.Module\n",
    "===\n",
    "\n",
    "The steps are:\n",
    "- define a class inheriting from the nn.Module\n",
    "- define a forward method\n",
    "- you don't have to define the backpropagation method !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a feed-forward network in PyTorch\n",
    "======================================\n",
    "\n",
    "Let's build a MLP in PyTorch, with a single hidden layer with a non-linear activation function, and softmax to get probabilities of classes.\n",
    "We'll then use the negative log likelihood loss, and stochastic gradient descent to learn the parameters.\n",
    "\n",
    "Because log-likelihood supposes to take the log of probabilities, in our network we will directly output log-softmax instead of softmax.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the network\n",
    "\n",
    "In pytorch, all network components should inherit from nn.Module and override the\n",
    "forward() method. \n",
    "\n",
    "Inheriting from nn.Module provides functionality to your\n",
    "component, like backpropagation and switching from CPU to GPU computing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAM named linear_1.weight, of shape torch.Size([10, 20])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1152, -0.0987, -0.0433,  0.1050, -0.2105,  0.1341, -0.0460,  0.1138,\n",
      "          0.0311, -0.0274,  0.0620,  0.0110,  0.0817, -0.0871, -0.0163, -0.0201,\n",
      "          0.0324, -0.0009,  0.1955,  0.0696],\n",
      "        [-0.0833, -0.1350, -0.0375, -0.0965, -0.0717,  0.0107,  0.1333,  0.1215,\n",
      "         -0.2186,  0.1386,  0.0625,  0.2121,  0.1476, -0.2037, -0.2126, -0.1079,\n",
      "          0.1964, -0.0372,  0.0957, -0.1039],\n",
      "        [ 0.2194, -0.0946,  0.1677,  0.0026, -0.1178,  0.1149, -0.1187,  0.0658,\n",
      "         -0.0646, -0.0245, -0.2150, -0.1066,  0.1213, -0.0544,  0.2227,  0.1792,\n",
      "         -0.0105, -0.1493,  0.1362,  0.0694],\n",
      "        [-0.1445,  0.1452,  0.1358,  0.1983, -0.1253, -0.0368, -0.0043,  0.0327,\n",
      "         -0.1697, -0.1587,  0.1216, -0.0524,  0.1092,  0.0127,  0.0734,  0.0492,\n",
      "          0.0813,  0.1108, -0.2071,  0.1126],\n",
      "        [-0.1572, -0.1687,  0.0136, -0.0381,  0.1313, -0.1295, -0.1988,  0.1627,\n",
      "         -0.0332,  0.1258,  0.0719, -0.1677,  0.0449,  0.0537, -0.1497, -0.1061,\n",
      "          0.0762,  0.0401, -0.0951, -0.0677],\n",
      "        [ 0.2048, -0.0414,  0.1261,  0.0968, -0.1445, -0.1902,  0.2146,  0.0117,\n",
      "          0.1533,  0.0463,  0.0719,  0.1670,  0.2120, -0.1484,  0.0280,  0.1669,\n",
      "          0.1620,  0.1389, -0.1618, -0.1610],\n",
      "        [-0.1352,  0.0281,  0.2229, -0.1412,  0.1192, -0.1238, -0.2102, -0.0475,\n",
      "          0.1288,  0.2076, -0.1389,  0.0485,  0.1929,  0.1482,  0.1394,  0.1589,\n",
      "          0.1414,  0.0578, -0.1529, -0.1878],\n",
      "        [-0.1025, -0.0260, -0.1371,  0.0818,  0.0692, -0.0506,  0.0859,  0.0723,\n",
      "          0.1365,  0.1506, -0.0757,  0.2185, -0.0259, -0.0077, -0.2111, -0.1439,\n",
      "         -0.1306, -0.0956,  0.1590, -0.0731],\n",
      "        [-0.1671,  0.0860,  0.0716,  0.1448, -0.1157,  0.0485, -0.0814, -0.0502,\n",
      "         -0.1782, -0.1019, -0.0685,  0.0956,  0.0409,  0.0552,  0.2232,  0.2179,\n",
      "          0.1525,  0.0071, -0.1547,  0.1748],\n",
      "        [-0.0559, -0.0181, -0.1926, -0.0442, -0.1442,  0.2055, -0.1933, -0.1743,\n",
      "         -0.0076, -0.1209,  0.0800, -0.0861, -0.1050,  0.0127,  0.1619, -0.1573,\n",
      "          0.1050,  0.1437,  0.2187, -0.1565]], requires_grad=True)\n",
      "PARAM named linear_1.bias, of shape torch.Size([10])\n",
      "Parameter containing:\n",
      "tensor([ 0.0542, -0.1653,  0.1909, -0.0867,  0.1347,  0.0067, -0.0174, -0.0071,\n",
      "         0.0380,  0.1054], requires_grad=True)\n",
      "PARAM named linear_2.weight, of shape torch.Size([3, 10])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0507,  0.0965, -0.2845,  0.2304,  0.2757,  0.2614,  0.2338, -0.2282,\n",
      "         -0.1172,  0.2788],\n",
      "        [-0.2409,  0.2869, -0.2487, -0.2228,  0.1546, -0.2272, -0.0724,  0.2301,\n",
      "          0.2505,  0.2991],\n",
      "        [-0.0642, -0.2458,  0.3114, -0.0674, -0.1301,  0.0771, -0.2211,  0.2079,\n",
      "          0.1982, -0.2509]], requires_grad=True)\n",
      "PARAM named linear_2.bias, of shape torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([-0.2597, -0.0277,  0.1328], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class MyMLP(nn.Module):  # inheriting from nn.Module\n",
    "\n",
    "    def __init__(self, nb_classes, d, hidden_layer_size, batch_size=1):\n",
    "        \"\"\" - d is the size of the vectors representing the objects to classify \n",
    "            - hidden_layer_size is ... the size of the hidden layer\n",
    "        \"\"\"\n",
    "        # calls the init function of the superclass, i.e. nn.Module\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        # The parameters of the network that must be learnt\n",
    "        # are inferred thanks to pytorch machinary, \n",
    "        # from what is declared in the constructor \n",
    "\n",
    "        # linear layer Wx + b\n",
    "        # => implicitely contains parameters W and b\n",
    "        # cf. https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "        # which are randomly initialized by default\n",
    "        self.linear_1 = nn.Linear(d, hidden_layer_size)\n",
    "        \n",
    "        # another pair of W and b parameters\n",
    "        self.linear_2 = nn.Linear(hidden_layer_size, nb_classes)\n",
    "\n",
    "        # NB: the non-linear activation functions do not have any parameter\n",
    "        # so we don't need to declare it in the __init__ method\n",
    "        # they will be used directly in the forward method\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Input : X batch of inputs : ( shape (b, d) )\n",
    "                with b is the size of the batch (it does not have to be constant)\n",
    "        Output : log probabilities (shape (b, nb_classes) )\n",
    "        \"\"\"\n",
    "        # linear combination from input to hidden layer\n",
    "        out = self.linear_1(X) # out shape (b, hidden_layer_size)\n",
    "        \n",
    "        # activation function at hidden layer\n",
    "        out = torch.relu(out) # same shape\n",
    "        \n",
    "        # linear combination from hidden to output layer\n",
    "        out = self.linear_2(out) # out shape (b, nb_classes)\n",
    "        \n",
    "        # Transformation into log-probabilities\n",
    "        #  Note : Since we're going to use the NLL loss, \n",
    "        #         we will need to apply log to softmax anyway\n",
    "        #         we do it here because directly computing log of softmax is numerically more stable\n",
    "        # CAUTION: the dim=1 is crucial here to tell pytorch on which axis apply the softmax\n",
    "        return F.log_softmax(out, dim=1) # out shape (n, nb_classes)\n",
    "    \n",
    "# examples\n",
    "my_classifier = MyMLP(nb_classes=3, d=20, hidden_layer_size=10, batch_size=2)\n",
    "\n",
    "# nn.Module and any subclass (e.g. nn.Linear, BoWClassifier)\n",
    "# records what are the parameters\n",
    "# depending on what has been declared in the __init__ method.\n",
    "\n",
    "# For nn.Linear, the first param is matrix W, the second is bias b.\n",
    "# Through some Python magic from the PyTorch devs, your module\n",
    "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
    "\n",
    "# and the parameters have requires_grad set to True of course\n",
    "for name, param in my_classifier.named_parameters():\n",
    "    print(\"PARAM named %s, of shape %s\" % (name, str(param.shape)))\n",
    "    print(param)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "The task we consider is **language identification**,\n",
    "and we are using a toy data set of 4 training examples, and 2 test examples (below).\n",
    "\n",
    "TODO\n",
    "==\n",
    "After you've programmed all the TODOs below,\n",
    "switch to the ep-08-04* files, which contain 500 sentences in French and 500 sentences in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "                  (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "                  (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "                  (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_examples = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "                 (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Encoding data : creating indices and converting data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE: 23\n",
      "NB CLASSES: 2\n",
      "w2i: {'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22}\n",
      "i2w: ['me', 'gusta', 'comer', 'en', 'la', 'cafeteria', 'Give', 'it', 'to', 'No', 'creo', 'que', 'sea', 'una', 'buena', 'idea', 'is', 'not', 'a', 'good', 'get', 'lost', 'at']\n",
      "label2i: {'SPANISH': 0, 'ENGLISH': 1}\n",
      "X_train torch.Size([4, 23])\n",
      "Y_train torch.Size([4])\n",
      "X_test torch.Size([2, 23])\n",
      "Y_test torch.Size([2])\n",
      "tensor([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# word to index\n",
    "# and label to index correspondances\n",
    "w2i = {}\n",
    "i2w = []\n",
    "label2i = {}\n",
    "i2label = []\n",
    "\n",
    "# TODO:\n",
    "for example in train_examples:\n",
    "  for word in example[0]: # example[0] is the sentence\n",
    "    if word not in w2i:\n",
    "      w2i[word] = len(w2i)\n",
    "      i2w.append(word)\n",
    "  label = example[1] # example[1] is the label\n",
    "  if label not in label2i:\n",
    "    label2i[label] = len(label2i)\n",
    "    i2label.append(label)\n",
    "    \n",
    "# fill in the w2i, i2w, label2i and i2label\n",
    "# using the training examples only\n",
    "# the words in test that are unknown in train will be ignored\n",
    "\n",
    "# instead YOU CAN USE the CountVectorizer of sklearn \n",
    "\n",
    "\n",
    "VOCAB_SIZE = len(w2i)\n",
    "NB_CLASSES = len(label2i)\n",
    "\n",
    "print(\"VOCAB SIZE:\", VOCAB_SIZE)\n",
    "print(\"NB CLASSES:\", NB_CLASSES)\n",
    "print(\"w2i:\", w2i)\n",
    "print(\"i2w:\", i2w)\n",
    "print(\"label2i:\", label2i)\n",
    "\n",
    "\n",
    "def convert_examples_to_tensors(examples, w2i, label2i):\n",
    "    \"\"\" Input = \n",
    "          - list of n examples\n",
    "             -- each example is a pair [sentence, class label]\n",
    "             -- a sentence being a list of tokens\n",
    "          - dictionary of words to indices\n",
    "          - dictionary of class labels to indices\n",
    "        Output = \n",
    "          - X = BOW vectors for the n examples:\n",
    "              = pytorch tensor of shape ( n , vocabulary size )\n",
    "                X[i, j] = number of occ of word j in sentence i\n",
    "          - Y = tensor of shape  ( n ) for indices of gold labels\n",
    "    \"\"\"\n",
    "    # separating input sentences and gold labels\n",
    "    (sentences, gold_labels) = list(zip(*examples))\n",
    "    \n",
    "    n = len(examples)\n",
    "    \n",
    "    vectorizer = CountVectorizer(vocabulary=w2i) # Initialize vectorizer with vocabulary\n",
    "    X_vec = vectorizer.transform([ \" \".join(sentence) for sentence in sentences ]) # Convert sentences to BOW vectors\n",
    "    X = torch.from_numpy(X_vec.toarray()).float() # Convert BOW vectors to tensor of shape (n, VOCAB_SIZE) and of type float\n",
    "    Y = torch.LongTensor([ label2i[gold_label] for gold_label in gold_labels ]) # Convert gold labels to tensor of indices of shape (n)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X_test = None\n",
    "X_train = None\n",
    "Y_test = None\n",
    "Y_train = None\n",
    "\n",
    "X_train, Y_train = convert_examples_to_tensors(train_examples, w2i, label2i)\n",
    "X_test, Y_test = convert_examples_to_tensors(test_examples, w2i, label2i)\n",
    "\n",
    "print(\"X_train\", X_train.size())\n",
    "print(\"Y_train\", Y_train.size())\n",
    "print(\"X_test\", X_test.size())\n",
    "print(\"Y_test\", Y_test.size())\n",
    "print(X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classifier = MyMLP(nb_classes=NB_CLASSES, d=VOCAB_SIZE, hidden_layer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of forward propagation (with random parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probabilities on test before training:\n",
      " tensor([[-0.6232, -0.7683],\n",
      "        [-0.5101, -0.9174]])\n",
      "PREDICTED LABELS ['SPANISH', 'SPANISH']\n",
      "     GOLD LABELS ['SPANISH', 'ENGLISH']\n"
     ]
    }
   ],
   "source": [
    "# Test of forward propagation with the randomly initialized parameters:\n",
    "\n",
    "with torch.no_grad():\n",
    "    # X_test : shape [n , vocab_size]\n",
    "    # NB: to run the forward method, use the name of the MyMLM instance\n",
    "    log_probs = my_classifier(X_test)  # shape [n , num_labels]\n",
    "    print(\"Log probabilities on test before training:\\n\", log_probs)\n",
    "\n",
    "    # prediction : argmax of the log_probabilities\n",
    "    # (make sure to control the axis on which the argmax is computed)\n",
    "    pred_labels = torch.argmax(log_probs, dim=1) # shape n\n",
    "    print(\"PREDICTED LABELS\", [ i2label[l] for l in pred_labels ])\n",
    "    print(\"     GOLD LABELS\", [ i2label[l] for l in Y_test ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "So lets train! To do this, we pass instances through forward propagation\n",
    "to get log probabilities, compute a loss function, compute the gradient of the loss\n",
    "function, and then update the parameters with a gradient step. \n",
    "\n",
    "Loss functions are provided by Torch in the nn package. nn.NLLLoss() is the\n",
    "negative log likelihood loss we want. \n",
    "\n",
    "Optimization functions are in torch.optim. Here, we will just use SGD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on:\n",
      "(['me', 'gusta', 'comer', 'en', 'la', 'cafeteria'], 'SPANISH')\n",
      "(['Give', 'it', 'to', 'me'], 'ENGLISH')\n",
      "(['No', 'creo', 'que', 'sea', 'una', 'buena', 'idea'], 'SPANISH')\n",
      "(['No', 'it', 'is', 'not', 'a', 'good', 'idea', 'to', 'get', 'lost', 'at', 'sea'], 'ENGLISH')\n",
      "\n",
      "\n",
      "{'SPANISH': 0, 'ENGLISH': 1}\n",
      "Epoch 0\n",
      "LOG PROBS on train at epoch 0:\n",
      " tensor([[-0.5860, -0.8131],\n",
      "        [-0.4698, -0.9812],\n",
      "        [-0.6020, -0.7934],\n",
      "        [-0.5899, -0.8083]], grad_fn=<LogSoftmaxBackward0>)\n",
      "PREDICTED LABELS ON BATCH ['SPANISH', 'SPANISH', 'SPANISH', 'SPANISH']\n",
      "     GOLD LABELS ON BATCH ['SPANISH', 'ENGLISH', 'ENGLISH', 'SPANISH']\n",
      "Epoch 1\n",
      "LOG PROBS on train at epoch 1:\n",
      " tensor([[-0.6379, -0.7516],\n",
      "        [-0.5843, -0.8153],\n",
      "        [-0.5779, -0.8234],\n",
      "        [-0.5283, -0.8906]], grad_fn=<LogSoftmaxBackward0>)\n",
      "PREDICTED LABELS ON BATCH ['SPANISH', 'SPANISH', 'SPANISH', 'SPANISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'SPANISH', 'SPANISH', 'ENGLISH']\n",
      "Epoch 2\n",
      "LOG PROBS on train at epoch 2:\n",
      " tensor([[-0.5739, -0.8286],\n",
      "        [-0.6621, -0.7252],\n",
      "        [-0.5756, -0.8264],\n",
      "        [-0.5674, -0.8370]], grad_fn=<LogSoftmaxBackward0>)\n",
      "PREDICTED LABELS ON BATCH ['SPANISH', 'SPANISH', 'SPANISH', 'SPANISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'SPANISH', 'SPANISH']\n",
      "Epoch 3\n",
      "LOG PROBS on train at epoch 3:\n",
      " tensor([[-0.5644, -0.8410],\n",
      "        [-0.6841, -0.7022],\n",
      "        [-0.5551, -0.8534],\n",
      "        [-0.6156, -0.7772]], grad_fn=<LogSoftmaxBackward0>)\n",
      "PREDICTED LABELS ON BATCH ['SPANISH', 'SPANISH', 'SPANISH', 'SPANISH']\n",
      "     GOLD LABELS ON BATCH ['SPANISH', 'ENGLISH', 'SPANISH', 'ENGLISH']\n",
      "Epoch 4\n",
      "LOG PROBS on train at epoch 4:\n",
      " tensor([[-0.7048, -0.6816],\n",
      "        [-0.5411, -0.8725],\n",
      "        [-0.5509, -0.8590],\n",
      "        [-0.6546, -0.7333]], grad_fn=<LogSoftmaxBackward0>)\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'SPANISH', 'SPANISH', 'SPANISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'SPANISH', 'SPANISH', 'ENGLISH']\n",
      "Epoch 5\n",
      "LOG PROBS on train at epoch 5:\n",
      " tensor([[-0.5257, -0.8944],\n",
      "        [-0.6910, -0.6953],\n",
      "        [-0.5355, -0.8804],\n",
      "        [-0.7247, -0.6626]], grad_fn=<LogSoftmaxBackward0>)\n",
      "PREDICTED LABELS ON BATCH ['SPANISH', 'SPANISH', 'SPANISH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['SPANISH', 'ENGLISH', 'SPANISH', 'ENGLISH']\n",
      "Epoch 6\n",
      "LOG PROBS on train at epoch 6:\n",
      " tensor([[-0.7418, -0.6467],\n",
      "        [-0.5166, -0.9077],\n",
      "        [-0.7157, -0.6711],\n",
      "        [-0.5091, -0.9189]], grad_fn=<LogSoftmaxBackward0>)\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'SPANISH', 'ENGLISH', 'SPANISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'SPANISH', 'ENGLISH', 'SPANISH']\n",
      "Epoch 7\n",
      "LOG PROBS on train at epoch 7:\n",
      " tensor([[-0.4880, -0.9516],\n",
      "        [-0.4928, -0.9439],\n",
      "        [-0.7270, -0.6604],\n",
      "        [-0.7474, -0.6417]], grad_fn=<LogSoftmaxBackward0>)\n",
      "PREDICTED LABELS ON BATCH ['SPANISH', 'SPANISH', 'ENGLISH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['SPANISH', 'SPANISH', 'ENGLISH', 'ENGLISH']\n",
      "Epoch 8\n",
      "LOG PROBS on train at epoch 8:\n",
      " tensor([[-0.7500, -0.6393],\n",
      "        [-0.4768, -0.9695],\n",
      "        [-0.7655, -0.6257],\n",
      "        [-0.4725, -0.9767]], grad_fn=<LogSoftmaxBackward0>)\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'SPANISH', 'ENGLISH', 'SPANISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'SPANISH', 'ENGLISH', 'SPANISH']\n",
      "Epoch 9\n",
      "LOG PROBS on train at epoch 9:\n",
      " tensor([[-0.4522, -1.0112],\n",
      "        [-0.4503, -1.0145],\n",
      "        [-0.7667, -0.6247],\n",
      "        [-0.7773, -0.6155]], grad_fn=<LogSoftmaxBackward0>)\n",
      "PREDICTED LABELS ON BATCH ['SPANISH', 'SPANISH', 'ENGLISH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['SPANISH', 'SPANISH', 'ENGLISH', 'ENGLISH']\n",
      "\n",
      "Prediction on test, after training:\n",
      "LOG PROBS on test:\n",
      " tensor([[-0.6092, -0.7848],\n",
      "        [-0.6582, -0.7294]])\n",
      "PREDICTED LABELS ['SPANISH', 'SPANISH']\n",
      "     GOLD LABELS ['SPANISH', 'ENGLISH']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --------- the loss -------------\n",
    "# negative log likelihood loss\n",
    "# TODO: check its input and output https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "loss_function = nn.NLLLoss() \n",
    "\n",
    "# --------- the optimizer --------\n",
    "# simplest one: stochastic gradient descent\n",
    "# we declare the parameters we wish to optimize  \n",
    "# => here we want to optimize all the parameters of our BoWClassifier instance\n",
    "optimizer = optim.SGD(my_classifier.parameters(), lr=0.1)\n",
    "\n",
    "print(\"\\nTraining on:\")\n",
    "for x in train_examples:\n",
    "    print(x)\n",
    "print(\"\\n\")\n",
    "print(label2i)\n",
    "\n",
    "# loop on epochs\n",
    "for epoch in range(10):\n",
    "    print(\"Epoch\", epoch)\n",
    "    shuffle(train_examples) # NB: here original order is lost\n",
    "\n",
    "    # NB: here if we had more examples,\n",
    "    # we should loop on mini batches (in random order)\n",
    "    # since we only have 4 training examples\n",
    "    # on each epoch we use a full batch of 4 training examples\n",
    "    \n",
    "    # Step 1:\n",
    "    # get tensors for batch of examples (here : one batch = all the training data)\n",
    "    X, Y = convert_examples_to_tensors(train_examples, w2i, label2i)\n",
    "\n",
    "\n",
    "    # Step 2: \n",
    "    # (re)sets all parameter gradients to 0\n",
    "    #  before using each batch of inputs\n",
    "    my_classifier.zero_grad()\n",
    "    \n",
    "    # Step 3: forward propagation\n",
    "    #         NB: my_classifier(X) implicitely calls my_classifier.forward(X)\n",
    "    log_probs = my_classifier(X)\n",
    "    \n",
    "    print(\"LOG PROBS on train at epoch %i:\\n\" %epoch, log_probs)\n",
    "\n",
    "    # optional: accuracy on this batch\n",
    "    pred_labels = torch.argmax(log_probs, dim=1)\n",
    "    print(\"PREDICTED LABELS ON BATCH\", [ i2label[l] for l in pred_labels ])\n",
    "    print(\"     GOLD LABELS ON BATCH\", [ i2label[l] for l in Y ])\n",
    "    \n",
    "    # Step 4: Compute the loss (NB: this is the loss for the full batch of inputs X)\n",
    "    #         The input to the loss function is for each example in X,\n",
    "    #          the log_probabilities for each class, and the gold label\n",
    "    loss = loss_function(log_probs, Y) \n",
    "\n",
    "    \n",
    "    # Step 5: Compute the gradients\n",
    "    loss.backward() # partial derivatives of loss with respect to\n",
    "                    # all the tensors that - were used to compute loss,\n",
    "                    #                      - and have requires_grad=True\n",
    "                    # after this call, all the parameters have their .grad attribute\n",
    "                    # filled with the partial derivative\n",
    "\n",
    "    # Step 6: Update the parameters\n",
    "    #         NB: the optimizer instance knows what are the parameters to update\n",
    "    optimizer.step()\n",
    "\n",
    "# prediction after training:\n",
    "print(\"\\nPrediction on test, after training:\")\n",
    "# NB: when we are not training, we don't have to compute gradients\n",
    "# => to be more efficient, we use torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    log_probs = my_classifier(X_test)  \n",
    "    print(\"LOG PROBS on test:\\n\", log_probs)\n",
    "    \n",
    "    pred_labels = torch.argmax(log_probs, dim=1)\n",
    "    print(\"PREDICTED LABELS\", [ i2label[l] for l in pred_labels ])\n",
    "    print(\"     GOLD LABELS\", [ i2label[l] for l in Y_test ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "==\n",
    "\n",
    "- implement the inner loop on batches (size of batch = hyperparameter)\n",
    "- implement early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 23]) torch.Size([2])\n",
      "torch.Size([2, 23]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# Define a batch function:\n",
    "def batchify(examples, batch_size):\n",
    "    for i in range(0, len(examples), batch_size):\n",
    "        yield examples[i:i+batch_size]\n",
    "\n",
    "# Test the batch function:\n",
    "batches = batchify(train_examples, 2)\n",
    "for batch in batches:\n",
    "    X, Y = convert_examples_to_tensors(batch, w2i, label2i)\n",
    "    print(X.size(), Y.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the batch size (inner loop) and the early stopping mechanism (step 7):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on:\n",
      "(['No', 'creo', 'que', 'sea', 'una', 'buena', 'idea'], 'SPANISH')\n",
      "(['me', 'gusta', 'comer', 'en', 'la', 'cafeteria'], 'SPANISH')\n",
      "(['No', 'it', 'is', 'not', 'a', 'good', 'idea', 'to', 'get', 'lost', 'at', 'sea'], 'ENGLISH')\n",
      "(['Give', 'it', 'to', 'me'], 'ENGLISH')\n",
      "\n",
      "\n",
      "{'SPANISH': 0, 'ENGLISH': 1}\n",
      "Epoch 0\n",
      "LOG PROBS on train at batch 0:\n",
      " tensor([[-0.5083, -0.9201]], grad_fn=<LogSoftmaxBackward0>)\n",
      "PREDICTED LABELS ON BATCH ['SPANISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH']\n",
      "Accuracy on batch: 0.00%\n",
      "LOG PROBS on train at batch 1:\n",
      " tensor([[-0.6386, -0.7508]], grad_fn=<LogSoftmaxBackward0>)\n",
      "PREDICTED LABELS ON BATCH ['SPANISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH']\n",
      "Accuracy on batch: 0.00%\n",
      "LOG PROBS on train at batch 2:\n",
      " tensor([[-0.6010, -0.7947]], grad_fn=<LogSoftmaxBackward0>)\n",
      "PREDICTED LABELS ON BATCH ['SPANISH']\n",
      "     GOLD LABELS ON BATCH ['SPANISH']\n",
      "Accuracy on batch: 100.00%\n",
      "LOG PROBS on train at batch 3:\n",
      " tensor([[-0.6208, -0.7711]], grad_fn=<LogSoftmaxBackward0>)\n",
      "PREDICTED LABELS ON BATCH ['SPANISH']\n",
      "     GOLD LABELS ON BATCH ['SPANISH']\n",
      "Accuracy on batch: 100.00%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dev_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m log_probs \u001b[39m=\u001b[39m my_classifier(X_test) \u001b[39m# Compute log probabilities on dev set\u001b[39;00m\n\u001b[0;32m     74\u001b[0m loss \u001b[39m=\u001b[39m loss_function(log_probs, Y_test) \u001b[39m# Compute loss on dev set\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m dev_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     76\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBest loss: \u001b[39m\u001b[39m{\u001b[39;00mbest_loss\u001b[39m:\u001b[39;00m\u001b[39m.4\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDev loss: \u001b[39m\u001b[39m{\u001b[39;00mdev_loss\u001b[39m:\u001b[39;00m\u001b[39m.4\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m at epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, batch \u001b[39m\u001b[39m{\u001b[39;00mnb_batch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dev_loss' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "my_classifier = MyMLP(nb_classes=NB_CLASSES, d=VOCAB_SIZE, hidden_layer_size=10, batch_size=1)\n",
    "# --------- the loss -------------\n",
    "# negative log likelihood loss\n",
    "# TODO: check its input and output https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "loss_function = nn.NLLLoss() \n",
    "\n",
    "# --------- the optimizer --------\n",
    "# simplest one: stochastic gradient descent\n",
    "# we declare the parameters we wish to optimize  \n",
    "# => here we want to optimize all the parameters of our BoWClassifier instance\n",
    "optimizer = optim.SGD(my_classifier.parameters(), lr=0.1)\n",
    "\n",
    "print(\"\\nTraining on:\")\n",
    "for x in train_examples:\n",
    "    print(x)\n",
    "print(\"\\n\")\n",
    "print(label2i)\n",
    "    \n",
    "# loop on epochs\n",
    "best_loss = float('inf') # Initialize best loss to infinity\n",
    "best_epoch = 0 # Initialize best epoch to 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(\"Epoch\", epoch)\n",
    "    shuffle(train_examples) # NB: here original order is lost\n",
    "    # loop on batches\n",
    "    batches = batchify(train_examples, batch_size=my_classifier.batch_size)\n",
    "    for nb_batch, batch  in enumerate(batches):\n",
    "        \n",
    "    # NB: here if we had more examples,\n",
    "    # we should loop on mini batches (in random order)\n",
    "    # since we only have 4 training examples\n",
    "    # on each epoch we use a full batch of 4 training examples\n",
    "    \n",
    "    # Step 1:\n",
    "    # get tensors for batch of examples (here : one batch = all the training data)\n",
    "        X, Y = convert_examples_to_tensors(batch, w2i, label2i)\n",
    "\n",
    "    # Step 2: \n",
    "    # (re)sets all parameter gradients to 0\n",
    "    #  before using each batch of inputs\n",
    "        my_classifier.zero_grad()\n",
    "    # Step 3: forward propagation\n",
    "    #         NB: my_classifier(X) implicitely calls my_classifier.forward(X)\n",
    "        log_probs = my_classifier(X)\n",
    "        print(\"LOG PROBS on train at batch %i:\\n\" %nb_batch, log_probs)\n",
    "    # optional: accuracy on this batch\n",
    "        pred_labels = torch.argmax(log_probs, dim=1)\n",
    "        accuracy = (pred_labels == Y).sum().item() / len(Y)\n",
    "        print(\"PREDICTED LABELS ON BATCH\", [ i2label[l] for l in pred_labels ])\n",
    "        print(\"     GOLD LABELS ON BATCH\", [ i2label[l] for l in Y ])\n",
    "        print(f\"Accuracy on batch: {accuracy:.2%}\")\n",
    "    \n",
    "    # Step 4: Compute the loss (NB: this is the loss for the full batch of inputs X)\n",
    "    #         The input to the loss function is for each example in X,\n",
    "    #          the log_probabilities for each class, and the gold label\n",
    "        loss = loss_function(log_probs, Y) \n",
    "\n",
    "    \n",
    "    # Step 5: Compute the gradients\n",
    "        loss.backward() # partial derivatives of loss with respect to\n",
    "                    # all the tensors that - were used to compute loss,\n",
    "                    #                      - and have requires_grad=True\n",
    "                    # after this call, all the parameters have their .grad attribute\n",
    "                    # filled with the partial derivative\n",
    "\n",
    "    # Step 6: Update the parameters\n",
    "    #         NB: the optimizer instance knows what are the parameters to update\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Step 7: Implement early stopping:\n",
    "    with torch.no_grad():\n",
    "        log_probs = my_classifier(X_test) # Compute log probabilities on dev set\n",
    "        loss = loss_function(log_probs, Y_test) # Compute loss on dev set\n",
    "        dev_loss += loss.item()\n",
    "        print(f\"Best loss: {best_loss:.4}\")\n",
    "        print(f\"Dev loss: {dev_loss:.4} at epoch {epoch}, batch {nb_batch}\")\n",
    "            \n",
    "    if dev_loss < best_loss: # If the loss on the dev set is lower than the best loss\n",
    "        best_loss = dev_loss # Update the best loss\n",
    "        best_epoch = epoch # Update the best epoch\n",
    "    else: # If the loss on the dev set is higher than the best loss\n",
    "        print(\"Performance decreases, stopping training\")\n",
    "        print(f\"Best epoch: {best_epoch}, best loss: {best_loss:.4}\")\n",
    "        break # Stop training       \n",
    "\n",
    "        \n",
    "\n",
    "# prediction after training:\n",
    "print(\"\\nPrediction on test, after training:\")\n",
    "# NB: when we are not training, we don't have to compute gradients\n",
    "# => to be more efficient, we use torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    log_probs = my_classifier(X_test)  \n",
    "    print(\"LOG PROBS on test:\\n\", log_probs)\n",
    "    \n",
    "    pred_labels = torch.argmax(log_probs, dim=1)\n",
    "    accuracy = (pred_labels == Y_test).sum().item() / len(Y_test)\n",
    "    print(\"PREDICTED LABELS\", [ i2label[l] for l in pred_labels ])\n",
    "    print(\"     GOLD LABELS\", [ i2label[l] for l in Y_test ])\n",
    "    print(f\"Accuracy on test set: {accuracy:.2%}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the architecture on the EP dataset:\n",
    "### Read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Resumption', 'of', 'the', 'session'], 'ENGLISH')\n",
      "(['Reprise', 'de', 'la', 'session'], 'FRENCH')\n",
      "Number of training examples: 1000\n",
      "Number of test examples: 200\n",
      "Number of dev examples: 200\n"
     ]
    }
   ],
   "source": [
    "# Apply the architecture on the EP dataset:\n",
    "# Define a reader for the EP EN and FR dataset:\n",
    "# Because we know a priori that the EN dataset has all sentences in English,\n",
    "# we can directly use ENGLISH as a gold label\n",
    "def read_examples_en(filename):\n",
    "    examples = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                sentence = line.split()\n",
    "                example = (sentence, \"ENGLISH\")\n",
    "                examples.append(example)\n",
    "    return examples\n",
    "\n",
    "# We do the same for the FR dataset, but we use the gold label \"FRENCH\"\n",
    "def read_examples_fr(filename):\n",
    "    examples = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                sentence = line.split()\n",
    "                example = (sentence, \"FRENCH\")\n",
    "                examples.append(example)\n",
    "    return examples\n",
    "\n",
    "# Read the data:\n",
    "en_train_examples = read_examples_en(\"ep-en.tok.train\")\n",
    "fr_train_examples = read_examples_fr(\"ep-fr.tok.train\")\n",
    "en_test_examples = read_examples_en(\"ep-en.tok.test\")\n",
    "fr_test_examples = read_examples_fr(\"ep-fr.tok.test\")\n",
    "en_dev_examples = read_examples_en(\"ep-en.tok.dev\")\n",
    "fr_dev_examples = read_examples_fr(\"ep-fr.tok.dev\")\n",
    "print(en_train_examples[0])\n",
    "print(fr_train_examples[0])\n",
    "# Merge the data:\n",
    "train_examples = en_train_examples + fr_train_examples\n",
    "test_examples = en_test_examples + fr_test_examples\n",
    "dev_examples = en_dev_examples + fr_dev_examples\n",
    "print(\"Number of training examples:\", len(train_examples))\n",
    "print(\"Number of test examples:\", len(test_examples))\n",
    "print(\"Number of dev examples:\", len(dev_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE: 4541\n",
      "NB CLASSES: 2\n",
      "X_train.shape: torch.Size([1000, 4541])\n",
      "Y_train.shape: torch.Size([1000])\n",
      "X_test.shape: torch.Size([200, 4541])\n",
      "Y_test.shape: torch.Size([200])\n",
      "X_dev.shape: torch.Size([200, 4541])\n",
      "Y_dev.shape: torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary:\n",
    "def build_vocab(train_examples):\n",
    "  \n",
    "  # word to index\n",
    "  # and label to index correspondances\n",
    "  w2i = {}\n",
    "  i2w = []\n",
    "  label2i = {}\n",
    "  i2label = []\n",
    "\n",
    "  for example in train_examples:\n",
    "    for word in example[0]: # example[0] is the sentence\n",
    "      if word not in w2i:\n",
    "        w2i[word] = len(w2i)\n",
    "        i2w.append(word)\n",
    "    label = example[1] # example[1] is the label\n",
    "    if label not in label2i:\n",
    "      label2i[label] = len(label2i)\n",
    "      i2label.append(label)\n",
    "    \n",
    "  # fill in the w2i, i2w, label2i and i2label\n",
    "  # using the training examples only\n",
    "  # the words in test that are unknown in train will be ignored\n",
    "\n",
    "  VOCAB_SIZE = len(w2i)\n",
    "  NB_CLASSES = len(label2i)\n",
    "  return w2i, i2w, label2i, i2label, VOCAB_SIZE, NB_CLASSES\n",
    "\n",
    "# Build the vocabulary:\n",
    "w2i, i2w, label2i, i2label, VOCAB_SIZE, NB_CLASSES = build_vocab(train_examples)\n",
    "print(\"VOCAB SIZE:\", VOCAB_SIZE)\n",
    "print(\"NB CLASSES:\", NB_CLASSES)\n",
    "\n",
    "# Convert examples to tensors:\n",
    "# We reuse the code from the previous part:\n",
    "X_train, Y_train = convert_examples_to_tensors(train_examples, w2i, label2i)\n",
    "shuffle(test_examples)\n",
    "shuffle(dev_examples)\n",
    "X_test, Y_test = convert_examples_to_tensors(test_examples, w2i, label2i)\n",
    "X_dev, Y_dev = convert_examples_to_tensors(dev_examples, w2i, label2i)\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"Y_train.shape:\", Y_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"Y_test.shape:\", Y_test.shape)\n",
    "print(\"X_dev.shape:\", X_dev.shape)\n",
    "print(\"Y_dev.shape:\", Y_dev.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the architecture on the EP dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on the EP dataset:\n",
      "Epoch 0\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 0: 59.52%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 1: 50.00%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH']\n",
      "Train accuracy on batch 2: 50.00%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH']\n",
      "Train accuracy on batch 3: 54.76%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH']\n",
      "Train accuracy on batch 4: 66.67%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 5: 92.86%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 6: 85.71%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "Train accuracy on batch 7: 71.43%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 8: 88.10%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "Train accuracy on batch 9: 95.24%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH']\n",
      "Train accuracy on batch 10: 92.86%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH']\n",
      "Train accuracy on batch 11: 95.24%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH']\n",
      "Train accuracy on batch 12: 88.10%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 13: 95.24%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH']\n",
      "Train accuracy on batch 14: 97.62%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH']\n",
      "Train accuracy on batch 15: 95.24%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 16: 97.62%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "Train accuracy on batch 17: 97.62%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH']\n",
      "Train accuracy on batch 18: 90.48%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "Train accuracy on batch 19: 100.00%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH']\n",
      "Train accuracy on batch 20: 95.24%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH']\n",
      "Train accuracy on batch 21: 95.24%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH']\n",
      "Train accuracy on batch 22: 95.24%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "Train accuracy on batch 23: 100.00%\n",
      "Best loss: inf\n",
      "Dev loss: 6.365 at epoch 0, batch 23\n",
      "Epoch 1\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 0: 100.00%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH']\n",
      "Train accuracy on batch 1: 100.00%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 2: 97.62%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 3: 92.86%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 4: 100.00%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "Train accuracy on batch 5: 100.00%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 6: 95.24%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH']\n",
      "Train accuracy on batch 7: 100.00%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH']\n",
      "Train accuracy on batch 8: 95.24%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH']\n",
      "Train accuracy on batch 9: 100.00%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "Train accuracy on batch 10: 97.62%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "Train accuracy on batch 11: 100.00%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH']\n",
      "Train accuracy on batch 12: 97.62%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "Train accuracy on batch 13: 92.86%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH']\n",
      "Train accuracy on batch 14: 95.24%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 15: 97.62%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH']\n",
      "Train accuracy on batch 16: 97.62%\n",
      "PREDICTED LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 17: 95.24%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 18: 100.00%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "Train accuracy on batch 19: 100.00%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "Train accuracy on batch 20: 95.24%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH']\n",
      "Train accuracy on batch 21: 100.00%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "Train accuracy on batch 22: 100.00%\n",
      "PREDICTED LABELS ON BATCH ['ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH']\n",
      "     GOLD LABELS ON BATCH ['ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH']\n",
      "Train accuracy on batch 23: 97.06%\n",
      "Best loss: 6.365\n",
      "Dev loss: 6.518 at epoch 1, batch 23\n",
      "Performance decreases, stopping training\n",
      "Best epoch: 0, best loss: 6.365\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model:\n",
    "my_classifier = MyMLP(nb_classes=NB_CLASSES, d=VOCAB_SIZE, hidden_layer_size=10, batch_size=42)\n",
    "# --------- the loss -------------\n",
    "# negative log likelihood loss\n",
    "# TODO: check its input and output https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "loss_function = nn.NLLLoss() \n",
    "\n",
    "# --------- the optimizer --------\n",
    "# simplest one: stochastic gradient descent\n",
    "# we declare the parameters we wish to optimize  \n",
    "# => here we want to optimize all the parameters of our BoWClassifier instance\n",
    "optimizer = optim.SGD(my_classifier.parameters(), lr=0.1)\n",
    "\n",
    "print(\"\\nTraining on the EP dataset:\")\n",
    "\n",
    "    \n",
    "# loop on epochs\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "for epoch in range(100):\n",
    "    print(\"Epoch\", epoch)\n",
    "    shuffle(train_examples) # NB: here original order is lost\n",
    "    # loop on batches\n",
    "    batches = batchify(train_examples, batch_size=my_classifier.batch_size)\n",
    "    for nb_batch, batch  in enumerate(batches):\n",
    "    # NB: here if we had more examples,\n",
    "    # we should loop on mini batches (in random order)\n",
    "    # since we only have 4 training examples\n",
    "    # on each epoch we use a full batch of 4 training examples\n",
    "    \n",
    "    # Step 1:\n",
    "    # get tensors for batch of examples (here : one batch = all the training data)\n",
    "        X, Y = convert_examples_to_tensors(batch, w2i, label2i)\n",
    "\n",
    "    # Step 2: \n",
    "    # (re)sets all parameter gradients to 0\n",
    "    #  before using each batch of inputs\n",
    "        my_classifier.zero_grad()\n",
    "    # Step 3: forward propagation\n",
    "    #         NB: my_classifier(X) implicitely calls my_classifier.forward(X)\n",
    "        log_probs = my_classifier(X)\n",
    "        #print(\"LOG PROBS on train at batch %i:\\n\" %nb_batch, log_probs)\n",
    "    # optional: accuracy on this batch\n",
    "        pred_labels = torch.argmax(log_probs, dim=1)\n",
    "        accuracy = (pred_labels == Y).sum().item() / len(Y)\n",
    "        print(\"PREDICTED LABELS ON BATCH\", [ i2label[l] for l in pred_labels ])\n",
    "        print(\"     GOLD LABELS ON BATCH\", [ i2label[l] for l in Y ])\n",
    "        print(f\"Train accuracy on batch {nb_batch}: {accuracy:.2%}\")\n",
    "    \n",
    "    # Step 4: Compute the loss (NB: this is the loss for the full batch of inputs X)\n",
    "    #         The input to the loss function is for each example in X,\n",
    "    #          the log_probabilities for each class, and the gold label\n",
    "        loss = loss_function(log_probs, Y) \n",
    "\n",
    "    \n",
    "    # Step 5: Compute the gradients\n",
    "        loss.backward() # partial derivatives of loss with respect to\n",
    "                    # all the tensors that - were used to compute loss,\n",
    "                    #                      - and have requires_grad=True\n",
    "                    # after this call, all the parameters have their .grad attribute\n",
    "                    # filled with the partial derivative\n",
    "\n",
    "    # Step 6: Update the parameters\n",
    "    #         NB: the optimizer instance knows what are the parameters to update\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Step 7: Implement early stopping:\n",
    "    with torch.no_grad():\n",
    "        log_probs = my_classifier(X_dev) # Compute log probabilities on dev set\n",
    "        loss = loss_function(log_probs, Y_dev) # Compute loss on dev set\n",
    "        dev_loss += loss.item()\n",
    "        print(f\"Best loss: {best_loss:.4}\")\n",
    "        print(f\"Dev loss: {dev_loss:.4} at epoch {epoch}, batch {nb_batch}\")\n",
    "            \n",
    "    if dev_loss < best_loss: # If the loss on the dev set is lower than the best loss\n",
    "        best_loss = dev_loss # Update the best loss\n",
    "        best_epoch = epoch # Update the best epoch\n",
    "    else: # If the loss on the dev set is higher than the best loss\n",
    "        print(\"Performance decreases, stopping training\")\n",
    "        print(f\"Best epoch: {best_epoch}, best loss: {best_loss:.4}\")\n",
    "        break # Stop training\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction on test, after training on full dataset:\n",
      "PREDICTED LABELS ['FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH']\n",
      "     GOLD LABELS ['FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH']\n",
      "Accuracy on test set: 97.00%\n",
      "PREDICTED LABELS ['FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH']\n",
      "     GOLD LABELS ['FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'FRENCH', 'FRENCH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'ENGLISH', 'FRENCH', 'ENGLISH', 'FRENCH', 'FRENCH']\n",
      "Accuracy on dev set: 98.00%\n"
     ]
    }
   ],
   "source": [
    "# prediction after training:\n",
    "print(\"\\nPrediction on test, after training on full dataset:\")\n",
    "# NB: when we are not training, we don't have to compute gradients\n",
    "# => to be more efficient, we use torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    log_probs = my_classifier(X_test)  \n",
    "    #print(\"LOG PROBS on test:\\n\", log_probs)\n",
    "    \n",
    "    pred_labels = torch.argmax(log_probs, dim=1)\n",
    "    accuracy = (pred_labels == Y_test).sum().item() / len(Y_test)\n",
    "    print(\"PREDICTED LABELS\", [ i2label[l] for l in pred_labels ])\n",
    "    print(\"     GOLD LABELS\", [ i2label[l] for l in Y_test ])\n",
    "    print(f\"Accuracy on test set: {accuracy:.2%}\")\n",
    "    \n",
    "    # prediction on dev set:\n",
    "    log_probs = my_classifier(X_dev)\n",
    "    \n",
    "    pred_labels = torch.argmax(log_probs, dim=1)\n",
    "    accuracy = (pred_labels == Y_dev).sum().item() / len(Y_dev)\n",
    "    print(\"PREDICTED LABELS\", [ i2label[l] for l in pred_labels ])\n",
    "    print(\"     GOLD LABELS\", [ i2label[l] for l in Y_dev ])\n",
    "    print(f\"Accuracy on dev set: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
